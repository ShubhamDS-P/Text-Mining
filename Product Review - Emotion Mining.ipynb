{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining\n",
    "### We will be performing the Emotion Mining on the reviews of the  Crucial 8 GB RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.amazon.in/Crucial-DDR4-Laptop-Memory-CT8G4SFRA266/product-reviews/B08C56KXQJ/ref=cm_cr_arp_d_paging_btm_next_2?ie=UTF8&reviewerType=all_reviews&pageNumber=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page = requests.get(link)\n",
    "#page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#page.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soup = bs(page.content, 'html.parser')\n",
    "#soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#names = soup.find_all('span', class_='a-profile-name')\t\t#search class on the webpage by right clicking on the name of customer and selecting inspect option.\n",
    "#names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"cust_name = []\n",
    "for i in range(0,len(names)):\n",
    "    cust_name.append(names[i].get_text())\n",
    "cust_name\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I realized that the contents are only taken from the single page and to solve the problem and take the content from further pages I have written the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_name = []\n",
    "for i in range(1,10):\n",
    "    ip = []\n",
    "    base_link = link+str(i)\n",
    "    page = requests.get(base_link)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    names = soup.find_all('span', class_='a-profile-name')\n",
    "    for j in range(0,len(names)):\n",
    "        ip.append(names[j].get_text())\n",
    "    cust_name = cust_name+ip\n",
    "cust_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cust_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_names = []\n",
    "for x in cust_name:\n",
    "    if x not in final_names:\n",
    "        final_names.append(x)\n",
    "final_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping review title \n",
    "Right click on the title and select inspect option, it will open a window where it shows the code and text of that particular page along with it's code.\n",
    "Click on the arrow at the left top corner of this new window and click on the review title, it will show the code for it in the window and you can find the tag name # and class for this title in the code line which we will be using for the scraping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = soup.find_all('span',class_='review-title') # it is for the reference original code is below after the description.\n",
    "#title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the proper title so you won't make mistake like selecting review title from only top review, you have to select title from the list of all reviews.\n",
    "If the title from the list of all review doesn't have the class then get to the parent tag of that line and select the class from there, that class name might be \n",
    "long because it also contains the text discription like font, size, bold, etc. so only select the content part for our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we will write the code line again\n",
    "#title = soup.find_all('a',class_='review-title-content')\n",
    "#title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#review_title = []\n",
    "#for i in range(0,len(title)):\n",
    "#    review_title.append(title[i].get_text())\n",
    "#review_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_title = []\n",
    "for i in range(1,10):\n",
    "    ip = []\n",
    "    base_link = link+str(i)\n",
    "    page = requests.get(base_link)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    title = soup.find_all('a',class_='review-title-content')\n",
    "    for j in range(0,len(title)):\n",
    "        ip.append(title[j].get_text())\n",
    "    review_title = review_title+ip\n",
    "review_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing '\\n' from the left side of the string\n",
    "review_title[:] = [titles.lstrip('\\n') for titles in review_title]\n",
    "review_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing '\\n' from the right side of the string\n",
    "review_title[:] = [titles.rstrip('\\n') for titles in review_title]\n",
    "review_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(review_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_review_title = []\n",
    "for x in review_title:\n",
    "    if x not in final_review_title:\n",
    "        final_review_title.append(x)\n",
    "final_review_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_review_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the rating\n",
    "#rating = soup.find_all('i',class_='review-rating')\n",
    "#rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the unnecessary part of the scraped data\n",
    "#rate = []\n",
    "#for i in range(0,len(rating)):\n",
    "#    rate.append(rating[i].get_text())\n",
    "#rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If anything getting repeated i.e. the first 2 rating or first rating in the web page then use pop to remove them.\n",
    "# As we know that the no of rating on the page are 10 so getting length of the string we will find out if anything is being repeated or not.\n",
    "#len(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After cross checking we realise that the first 2 rating are being repeated so let's remove them.\n",
    "# Removing repeating element at the 0th position\n",
    "#rate.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing repeating element at the original 1st position which becomes 0th element after previouse pop\n",
    "#rate.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(rate)\n",
    "#len(rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating = []\n",
    "for i in range(1,10):\n",
    "    ip = []\n",
    "    base_link = link+str(i)\n",
    "    page = requests.get(base_link)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    rate = soup.find_all('i',class_='review-rating')\n",
    "    for j in range(0,len(rate)):\n",
    "        ip.append(rate[j].get_text())\n",
    "    rating = rating+ip\n",
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 methods with which we can find the text from the web page.\n",
    "1. In this method we use the same technique and steps which are shown above for finding the title, review title and rating.\n",
    "2. For the second method we will be using the data hooks from the inspector's window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the review texts\n",
    "# We will search the text data using data hook.\n",
    "#review = soup.find_all(\"span\",{\"data-hook\":\"review-body\"})\n",
    "#review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rmoving the un-necessary part from the imported data.\n",
    "#review_content = []\n",
    "#for i in range(0,len(review)):\n",
    "#    review_content.append(review[i].get_text())\n",
    "#review_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_content = []\n",
    "for i in range(1,10):\n",
    "    ip = []\n",
    "    base_link = link+str(i)\n",
    "    page = requests.get(base_link)\n",
    "    soup = bs(page.content, 'html.parser')\n",
    "    review = soup.find_all(\"span\",{\"data-hook\":\"review-body\"})\n",
    "    for j in range(0,len(review)):\n",
    "        ip.append(review[j].get_text())\n",
    "    review_content = review_content+ip\n",
    "review_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \\n from left side of the text\n",
    "review_content[:] = [reviews.lstrip('\\n') for reviews in review_content]\n",
    "review_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the \\n from right side of the text\n",
    "review_content[:] = [reviews.rstrip('\\n') for reviews in review_content]\n",
    "review_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(review_content)   #checking if there is any extra reviews which got repeated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews = []\n",
    "for x in review_content:\n",
    "    if x not in final_reviews:\n",
    "        final_reviews.append(x)\n",
    "final_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_review_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the data in the csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Saving the data we have to convert it into the dataframe first\n",
    "df = pd.DataFrame() # It will create the blank dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add the data into the dataframe according to it's specific row.\n",
    "df['Reviews']= final_reviews     # Assigning the data to reviews column in df\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sr. No.'] = np.arange(len(df))\n",
    "df = df[['Sr. No.','Reviews']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting to the csv file format\n",
    "df.to_csv(r'D:\\Data Science study\\Assignment of Data Science\\Sent\\17 Text Mining\\Reviews.csv',index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's work on the analysis part now\n",
    "Now we will perform the sentiment analysis on the data that we have retrieved from the Amazon [product reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = pd.read_csv(\"D:\\\\Data Science study\\\\Assignment of Data Science\\\\Sent\\\\17 Text Mining\\\\Reviews.csv\")\n",
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide what are the useful columns for data analysis so we can decide to use them and drop the unnecessary ones.\n",
    "# Let's check out the data first.\n",
    "data_file.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file.Reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will first work on the text processing where we will remove the stop words and unnecessary content from the text data\n",
    "# For that we will use 'textblob'\n",
    "! pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "# Lower casing and removing punctuations\n",
    "data_file['Reviews'] = data_file['Reviews'].apply(lambda x:\" \".join(x.lower() for x in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the punctuation\n",
    "data_file['Reviews'] = data_file['Reviews'].str.replace('[^\\w\\s]',\"\")\n",
    "data_file.Reviews.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removal of stop words\n",
    "stop = stopwords.words('english')\n",
    "data_file['Reviews'] = data_file['Reviews'].apply(lambda x: \" \".join(x for x in\n",
    "x.split() if x not in stop))\n",
    "data_file.Reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spelling correction\n",
    "data_file['Reviews'] = data_file['Reviews'].apply(lambda x: str(TextBlob(x). correct()))\n",
    "data_file.Reviews.head(5)    # This can also takes long time to execute as depending on your dataframe size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will work on the Lemmatization\n",
    "data_file['Reviews'] = data_file['Reviews'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "data_file.Reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will implement the processed data in to the word cloud so we can have a better understanding regarding the prominent terms in the data\n",
    "# To install wordcloud\n",
    "! pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file.dropna(inplace = True)\n",
    "data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reviews_str = []\n",
    "for x in data_file['Reviews']:\n",
    "    reviews_str.append(x)\n",
    "reviews_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(reviews_str)  # This is actually not a string but a list so let's convert this into the string so we can apply wordcloud to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_reviews_str = ' '.join([str(x) for x in reviews_str])\n",
    "final_reviews_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color='white').generate(final_reviews_str)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud,interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the VADER Sentiment for the sentiment analysis\n",
    "# First install the VADER Sentiment\n",
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "plt.style.use('fivethirtyeight')\n",
    "# Funtion for getting the sentiment\n",
    "cp = sns.color_palette()\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating sentiment for all the sentence present in the dataset\n",
    "emptyline=[]\n",
    "for row in data_file['Reviews']:\n",
    "    vs = analyzer.polarity_scores(row)\n",
    "    emptyline.append(vs)\n",
    "# Creating new dataframe with sentiments\n",
    "data_file_sentiments = pd.DataFrame(emptyline)\n",
    "data_file_sentiments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the sentiments back to reviews dataframe\n",
    "df_c = pd.concat([data_file.reset_index(drop = True), data_file_sentiments], axis = 1)\n",
    "df_c.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scores into positive and negative sentiments using some threshold\n",
    "df_c['Sentiment'] = np.where(df_c['compound'] >= 0, 'Positive', 'Negative')\n",
    "df_c.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c['Sentiment'].describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have extracted the reviews for the product \"8 GB Crucial RAM\", and we have performed sentiment analysis using VADER.\n",
    "We have also plotted a wordcloud for the extracted reviews. \n",
    "The DataFrame \"df_c\" shows the particular review and its sentiment scale and also tells us if the sentiment is positive, negative or neutral."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
